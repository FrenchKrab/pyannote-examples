{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only works for non permutation invariant tasks. Permutation invariant tasks (such as speaker diarization) have the problem of mismatched classes between the windows outputted by the sliding window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/2ffce0501d0aecad81b43a06d538186e292d0070/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.0. Bad things might happen unless you revert torch to 1.x.\n",
      "Specifications(problem=<Problem.MULTI_LABEL_CLASSIFICATION: 2>, resolution=<Resolution.FRAME: 1>, duration=5.0, min_duration=None, warm_up=(0.0, 0.0), classes=['speaker#1', 'speaker#2', 'speaker#3'], powerset_max_classes=None, permutation_invariant=True)\n",
      "Specifications(problem=<Problem.MONO_LABEL_CLASSIFICATION: 1>, resolution=<Resolution.FRAME: 1>, duration=5, min_duration=5, warm_up=(0.0, 0.0), classes=['nothing', 'ov'], powerset_max_classes=None, permutation_invariant=False)\n"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Model\n",
    "from pyannote.audio.models.segmentation import PyanNet\n",
    "from pyannote.audio.tasks.segmentation.multilabel import MultiLabelSegmentation\n",
    "from pyannote.database import registry, FileFinder\n",
    "from pathlib import Path\n",
    "\n",
    "registry.load_database(\"../_data/sample/sample.yaml\")\n",
    "protocol = registry.get_protocol(\"Sample.SpeakerDiarization.Debug\", preprocessors={\"audio\": FileFinder()})\n",
    "\n",
    "# authtoken = Path(\"../auth_token.txt\").read_text().strip()\n",
    "task = MultiLabelSegmentation(protocol, \"vad\")\n",
    "model = PyanNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 293, 3)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyannote.audio import Inference\n",
    "\n",
    "f = next(protocol.train())\n",
    "inf = Inference(model, batch_size=64, step=0.5, duration=model.specifications.duration)\n",
    "output = inf(f)\n",
    "output.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.core import Annotation, Timeline, Segment, SlidingWindow\n",
    "import torch\n",
    "import itertools\n",
    "\n",
    "reference: Annotation = f[\"annotation\"]\n",
    "uem: Timeline = f[\"annotated\"]\n",
    "\n",
    "# full file length\n",
    "support = Segment(0.0, uem.extent().end)\n",
    "\n",
    "#Â if not permutation invariant, the output is already (n_frames, n_classes)-shaped\n",
    "if len(output.data.shape) == 2:\n",
    "    pred = output.data\n",
    "# if permutation invariant, the aggregation couldn't be done. The data is (n_chunks, n_frames_per_chunk, n_classes)-shaped\n",
    "# however we dont know how to make the chunks (windows) match, so we cant aggregate\n",
    "else:\n",
    "    raise Exception()\n",
    "\n",
    "# get the targets tensor\n",
    "ref_t = reference.discretize(\n",
    "    support=support,\n",
    "    resolution=model.example_output.frames,\n",
    ").data\n",
    "ref_t = torch.from_numpy(ref_t).long()\n",
    "\n",
    "# get the uem boolean tensor\n",
    "uem_t = torch.from_numpy(uem.support().to_annotation().rename_labels(generator=itertools.cycle([\"uem\"])).discretize(\n",
    "    support=support,\n",
    "    resolution=model.example_output.frames,\n",
    "    labels=[\"uem\"],\n",
    ").data).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        ...,\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False],\n",
       "        [False],\n",
       "        [False],\n",
       "        ...,\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uem_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. ],\n",
       "       ...,\n",
       "       [0.8, 0.8, 0. ],\n",
       "       [0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. ]], dtype=float32)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.round(decimals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1759, 3), torch.Size([1758, 2]), torch.Size([1758, 1]))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred.shape, ref_t.shape, uem_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there might still be some frame count mismatches, due to float inaccuracies.\n",
    "\n",
    "Also, even if we cut the last frame of pred, there is still one row of [0., 0. ,0.], which come from NaNs. \n",
    "This is because wasn't any data found at these frames, and empty frames are NaNs (but the aggregation replaces NaN with zeros so that the output is at least workable).\n",
    "\n",
    "So here we might actually want to use (1757, x) shaped tensors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
